# CS3219 OTOT Task A3

* **Name**: Yusuf Bin Musa
* **Matric. Number**: A0218228E
* **Repo Link**: https://github.com/yusufaine/OTOT-A2-A3

---

## Task 3.1: Deploy `metrics-server` and `HorizontalPodAutoscale`

### Creation of `metrics-server`

While `a2-demo` is still running,  `metrics-server` can be created by executing the following command.

```bash
# Apply metrics-server to current cluster
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Allow TLS
kubectl -nkube-system edit deploy/metrics-server
# add `--kubelet-insecure-tls` to `deployment.spec.containers[].args[]`
# apply config change by restarting
kubectl -nkube-system rollout restart deploy/metrics-server

# Alternatively, download `components.yaml`, add the flag directly, and apply

# Verify that it is running
kubectl get pods --all-namespaces | grep metric
```

### Creation of `HorizontalPodAutoscale`

To create `HorizontalPodAutoscale`, we have to apply the following manifest.

```yaml
# hpa-nodejs-deploy-a2-backend.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend
  namespace: default
spec:
  metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 50
          type: Utilization
      type: Resource
  minReplicas: 1
  maxReplicas: 10
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment 
    name: backend # dependent on the container that needs scaling
```

This can then be applied and verified with the following commands.

```bash
kubectl apply -f ./hpa-nodejs-deploy-a2-backend.yaml

# Verify that it's connected to the metric server and running properly.
kubectl describe hpa

# Test scaling up in 2 terminals.
## First terminal watches for changes
while true; clear; do kubectl describe hpa; sleep 5; done

## Second terminal to stress-test
curl "localhost/?[0-4999]" --parallel --parallel-max 100 --no-progress-meter > /dev/null

## After awhile, 10 replicas should be created. 

# Test scaling down, wait for CPU usage to decrease and replicas to 
# terminate until only 1 is left. Refer to screenshots below
```

### HPA in action

Take note of the CPU usage.

#### Scaling up (stress-test not shown)
![Scaling up](https://i.ibb.co/bNGGzbb/telegram-cloud-photo-size-5-6093722655405487375-y.jpg)

#### Scaling down
![Scaling down](https://i.ibb.co/jyp0LNH/telegram-cloud-photo-size-5-6093722655405487376-y.jpg)

## Task 3.2: Zone-aware deployment of A2

To make A2 zone-aware, we first would need to modify some of its manifests, predominantly the `deploy.yaml` which was also provided as seen below. The `podAntiAffinity` configuration was also added to show that the pods are spread within the zone as well. Without it, pods created in `zone A` were created by `worker-2`.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-zone-aware
  labels:
    app: backend-zone-aware
spec:
  replicas: 10
  selector:
    matchLabels:
      app: backend-zone-aware
  template:
    metadata:
      labels:
        app: backend-zone-aware
    spec:
      containers:
        - name: backend
          image: yusufaine/nodejs-app:latest
          ports:
            - name: http
              containerPort: 3000
          resources:
            limits:
              cpu: 40m
              memory: 100Mi
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: backend-zone-aware
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                        - backend-zone-aware
                topologyKey: "kubernetes.io/hostname"
```

The `service`, and `ingress-object` would also need to update their labels and `metadata.name` and `metadata.labels` and apply them accordingly. Steps to apply them are similar to A2 and only the verification would be shown.

```bash
# Show workers and zones
kubectl get nodes -L topology.kubernetes.io/zone

# Show distribution of pods in zones
kubectl get po -lapp=backend-zone-aware -owide --sort-by='.spec.nodeName'
```

### High availability verification

![High availability achieved](https://i.ibb.co/1Q9bsQ9/image.png)

---

While not directly related to what we needed to do, I found it quite amazing that despite how HPA allows our app to scale dynamically, it would still not beat the performance of a service with high availability.

Not-so-scientific difference between scaling and HA.

```bash
time -h curl -s "localhost/?[0-4999]" --parallel --parallel-max 100 --no-progress-meter > /dev/null

# Task 3.1 (Scale from 1)
# 3m7.57s real  3.12s user  2.87s sys

# Task 3.2 (High availability)
# 54.48s real  3.03s user  2.77s sys
```
